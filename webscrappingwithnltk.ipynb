{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "webscrapping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOl8aE/3IBXeyP8rioNCL5m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivangeeSingh/artificial-inteligence/blob/master/webscrappingwithnltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0vrw1VBYU_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib import request # for downloading data from url\n",
        "from bs4 import BeautifulSoup # for souping\n",
        "import time\n",
        "import re # importing regular expression\n",
        "# pointing to url \n",
        "url='https://en.wikipedia.org/wiki/Machine_learning'\n",
        "\n",
        "htmldata=request.urlopen(url)\n",
        "#htmldata.read()  # it will download data in html format\n",
        "soupdata=BeautifulSoup(htmldata,'html5lib')\n",
        "#     html data , html parser ---\n",
        "#html parser is a collection of tags that can scrape data from particular tag like h1,html, p,a\n",
        "# now selecting a particular tag for data scrape\n",
        "atagdata=soupdata.findAll('p')\n",
        "#print(atagdata)\n",
        "# now converting data into string format from HTML format\n",
        "\n",
        "mydata=\"\"\n",
        "for i in atagdata:\n",
        "  mydata += i.text\n",
        "\n",
        "#print(mydata)\n",
        "\n",
        "# data cleaning\n",
        "#daat cleaning using regular expression REGEX\n",
        "clean_data=re.sub(r'\\[[0-9]*\\]',' ',mydata) # this will remove 0 or more times no appear in mydata\n",
        "clean_data=re.sub(r'\\s+',' ',clean_data) # remove more than 1 white spaces\n",
        "clean_data=re.sub(r'[^a-zA-Z]',' ',clean_data)# remove single char\n",
        "clean_data=re.sub(r'\\s+',' ',clean_data) # remove more than 1 white spaces\n",
        "print(clean_data)\n",
        "\n",
        "# now lets deal with string\n",
        "import time\n",
        "newd=clean_data.split() # split means conversion of str in list\n",
        "for words in newd:\n",
        "  print(words)\n",
        "  time.sleep(1)\n",
        "\n",
        "# now with data through nlp using nltk i.e. natural language tool kit\n",
        "import nltk\n",
        "import time\n",
        "newdata = clean_data.split()\n",
        "# downloading all dictionary for computer\n",
        "# nltk.download('all')\n",
        "#first remove the stop words like is, a, the\n",
        "mypuredata=[]\n",
        "from nltk.corpus import stopwords\n",
        "for i in newdata:\n",
        "    if i.lower() not in stopwords.words('english'):\n",
        "      mypuredata.append(i)\n",
        "\n",
        "print(mypuredata)\n",
        "freq_data=nltk.FreqDist(mypuredata)\n",
        "freq_data.plot(20) # this will give us the top 20 keywords having most count"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}